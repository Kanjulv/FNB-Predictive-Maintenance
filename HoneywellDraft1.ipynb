{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4Z492Y_J9BH",
        "outputId": "d98a6c75-193a-4464-d23c-8a63b9878483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Installing libraries...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m700.8/700.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüè≠ Generating enhanced data for SKU: Chocolate Chip...\n",
            "üè≠ Generating enhanced data for SKU: Oatmeal Raisin...\n",
            "ü§ñ Training models on data with original anomaly frequency...\n",
            "Training anomaly detection model on 4800 'normal' data points.\n",
            "‚úÖ Original data generated and models trained. Ready to launch.\n"
          ]
        }
      ],
      "source": [
        "# --- Original Setup Script (v4.0 - Probabilistic Anomaly) ---\n",
        "\n",
        "# 1. Install necessary libraries\n",
        "print(\"üöÄ Installing libraries...\")\n",
        "!pip install streamlit streamlit-autorefresh pyngrok joblib plotly -q\n",
        "\n",
        "# 2. Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import chi2\n",
        "import os\n",
        "\n",
        "# 3. Enhanced Data Generation Function with more parameters\n",
        "def generate_sku_data(sku_name, num_normal_batches=10, samples_per_batch=300):\n",
        "    print(f\"üè≠ Generating enhanced data for SKU: {sku_name}...\")\n",
        "\n",
        "    # Define SKU-specific golden profiles\n",
        "    if sku_name == \"Chocolate Chip\": base_temp, base_protein, base_airflow = 180, 12.5, 15.0\n",
        "    elif sku_name == \"Oatmeal Raisin\": base_temp, base_protein, base_airflow = 175, 11.8, 14.5\n",
        "    else: base_temp, base_protein, base_airflow = 180, 12.5, 15.0\n",
        "\n",
        "    # Inner function to generate a single batch\n",
        "    def generate_single_batch(batch_id, is_golden=False, has_anomaly=False):\n",
        "        time_steps = np.arange(samples_per_batch)\n",
        "        temp_profile = base_temp + 20 * np.sin(np.pi * time_steps / samples_per_batch)\n",
        "        airflow_profile = base_airflow + 1 * np.sin(0.5*np.pi*time_steps/samples_per_batch) # Dynamic airflow\n",
        "        protein_profile = base_protein * np.ones(samples_per_batch)\n",
        "\n",
        "        noise = 0.5 if is_golden else 1.0\n",
        "        temp_actual = temp_profile + np.random.normal(0, noise * 0.5, samples_per_batch)\n",
        "        pressure_actual = 2.5 + 0.1 * np.sin(2*np.pi*time_steps/samples_per_batch) + np.random.normal(0, noise*0.1, samples_per_batch)\n",
        "        airflow_actual = airflow_profile + np.random.normal(0, noise * 0.2, samples_per_batch)\n",
        "        raw_material_protein = protein_profile + np.random.normal(0, noise * 0.05, samples_per_batch)\n",
        "\n",
        "        if has_anomaly:\n",
        "            anomaly_type = np.random.choice(['low_protein', 'high_temp', 'low_airflow'])\n",
        "            if anomaly_type == 'low_protein': raw_material_protein -= 1.5\n",
        "            elif anomaly_type == 'high_temp': temp_actual[100:200] += 5\n",
        "            else: airflow_actual[50:150] -= 2.0\n",
        "\n",
        "        moisture = 8.0 - 0.05*(temp_actual - base_temp) + 0.5*(raw_material_protein - base_protein) - 0.1*(airflow_actual - base_airflow) + np.random.normal(0, 0.1, samples_per_batch)\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            'batch_id': batch_id, 'timestamp': pd.to_datetime(pd.Timestamp.now().normalize() + pd.to_timedelta(time_steps, unit='s')),\n",
        "            'temp_setpoint': temp_profile, 'temp_actual': temp_actual,\n",
        "            'pressure_actual': pressure_actual, 'airflow_setpoint': airflow_profile, 'airflow_actual': airflow_actual,\n",
        "            'raw_material_protein': raw_material_protein, 'product_moisture_qc': moisture,\n",
        "            'is_anomaly_source': has_anomaly\n",
        "        })\n",
        "\n",
        "    # Generate Golden, Demo (with potential anomaly), and Training batches\n",
        "    golden_df = generate_single_batch('golden', is_golden=True)\n",
        "    demo_df = generate_single_batch('live_demo', has_anomaly=True) # Anomaly is probabilistic\n",
        "    training_data_list = [golden_df] + [generate_single_batch(i, has_anomaly=(i % 4 == 0)) for i in range(num_normal_batches)]\n",
        "\n",
        "    for df in [golden_df, demo_df] + training_data_list:\n",
        "        df['temp_deviation'] = df['temp_actual'] - df['temp_setpoint']\n",
        "        df['airflow_deviation'] = df['airflow_actual'] - df['airflow_setpoint']\n",
        "\n",
        "    sku_prefix = sku_name.lower().replace(\" \", \"_\")\n",
        "    golden_df.to_csv(f'golden_{sku_prefix}.csv', index=False)\n",
        "    demo_df.to_csv(f'demo_{sku_prefix}.csv', index=False)\n",
        "    return pd.concat(training_data_list)\n",
        "\n",
        "# --- Main Execution ---\n",
        "sku_list = [\"Chocolate Chip\", \"Oatmeal Raisin\"]\n",
        "combined_training_data = pd.concat([generate_sku_data(sku) for sku in sku_list])\n",
        "\n",
        "print(\"ü§ñ Training models on data with original anomaly frequency...\")\n",
        "features = ['temp_actual', 'pressure_actual', 'raw_material_protein', 'airflow_actual', 'temp_deviation', 'airflow_deviation']\n",
        "target = 'product_moisture_qc'\n",
        "\n",
        "# Create a dataframe containing ONLY normal operating data for the statistical model\n",
        "normal_batches_df = combined_training_data[combined_training_data['is_anomaly_source'] == False].copy()\n",
        "print(f\"Training anomaly detection model on {len(normal_batches_df)} 'normal' data points.\")\n",
        "\n",
        "# Train the scaler and covariance matrix ONLY on the normal data\n",
        "scaler = StandardScaler().fit(normal_batches_df[features])\n",
        "cov_matrix = np.cov(scaler.transform(normal_batches_df[features]), rowvar=False)\n",
        "\n",
        "# The XGBoost model is trained on ALL data\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100).fit(combined_training_data[features], combined_training_data[target])\n",
        "\n",
        "# Save all objects\n",
        "joblib.dump(xgb_model, 'xgb_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(np.linalg.inv(cov_matrix), 'inv_cov_matrix.pkl')\n",
        "joblib.dump(features, 'features.pkl')\n",
        "joblib.dump(chi2.ppf(0.99, df=len(features)), 't2_threshold.pkl')\n",
        "\n",
        "print(\"‚úÖ Original data generated and models trained. Ready to launch.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import time\n",
        "import plotly.graph_objects as go\n",
        "import shap\n",
        "import xgboost\n",
        "from streamlit_autorefresh import st_autorefresh\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"F&B Predictive Maintenance System\")\n",
        "\n",
        "# --- Load Models & Static Assets ---\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    return {\n",
        "        \"xgb_model\": joblib.load('xgb_model.pkl'), \"scaler\": joblib.load('scaler.pkl'),\n",
        "        \"inv_cov_matrix\": joblib.load('inv_cov_matrix.pkl'), \"features\": joblib.load('features.pkl'),\n",
        "        \"t2_threshold\": joblib.load('t2_threshold.pkl'), \"explainer\": shap.TreeExplainer(joblib.load('xgb_model.pkl'))\n",
        "    }\n",
        "models = load_models()\n",
        "\n",
        "# --- Dynamic Data Loading based on SKU ---\n",
        "@st.cache_data\n",
        "def load_sku_data(sku_name):\n",
        "    sku_prefix = sku_name.lower().replace(\" \", \"_\")\n",
        "    data_files = {\n",
        "        \"live_demo_batch\": pd.read_csv(f'demo_{sku_prefix}.csv', parse_dates=['timestamp']),\n",
        "        \"golden_batch_df\": pd.read_csv(f'golden_{sku_prefix}.csv', parse_dates=['timestamp'])\n",
        "    }\n",
        "    for name, df in data_files.items():\n",
        "        if 'temp_deviation' not in df.columns: df['temp_deviation'] = df['temp_actual'] - df['temp_setpoint']\n",
        "        if 'airflow_deviation' not in df.columns: df['airflow_deviation'] = df['airflow_actual'] - df['airflow_setpoint']\n",
        "    return data_files\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def calculate_health_score(t2_score, t2_threshold):\n",
        "    health = 100 * (1 - (t2_score / (t2_threshold * 3)))\n",
        "    return max(0, min(100, health))\n",
        "\n",
        "def get_alert_details(data_point):\n",
        "    top_driver_index = np.argmax(np.abs(models[\"explainer\"](data_point[models[\"features\"]]).values))\n",
        "    top_driver = models[\"features\"][top_driver_index]\n",
        "    t2_score = (models['scaler'].transform(data_point[models['features']]) @ models['inv_cov_matrix'] @ models['scaler'].transform(data_point[models['features']]).T)[0][0]\n",
        "\n",
        "    if \"protein\" in top_driver: impact = f\"**Projected Yield Loss: {(0.5 + (t2_score / models['t2_threshold']) * 2):.1f}%**\"\n",
        "    else: impact = f\"**Projected Cpk Drift: {max(0.8, 1.33 - (t2_score / models['t2_threshold']) * 0.2):.2f}** (Target: >1.33)\"\n",
        "\n",
        "    cause = f\"Primary Driver: **{top_driver.replace('_', ' ').title()}**.\"\n",
        "    playbook = {\"step_1\": \"Acknowledge the alert and verify process area.\", \"step_2\": \"Notify Shift Supervisor of the deviation.\"}\n",
        "    if \"temp\" in top_driver: playbook[\"step_3\"] = \"Check oven heating elements and thermostats.\"\n",
        "    elif \"protein\" in top_driver: playbook[\"step_3\"] = \"Contact QA to verify raw material lot ID.\"\n",
        "    elif \"airflow\" in top_driver: playbook[\"step_3\"] = \"Inspect fans, filters, and ductwork for blockages.\"\n",
        "    else: playbook[\"step_3\"] = \"Follow standard procedure for pressure deviations.\"\n",
        "    return {\"cause\": cause, \"impact\": impact, \"playbook\": playbook}\n",
        "\n",
        "# --- Initialize Session State ---\n",
        "if 'sku' not in st.session_state:\n",
        "    st.session_state.sku = \"Chocolate Chip\"\n",
        "    st.session_state.running = False\n",
        "    st.session_state.step = 0\n",
        "    st.session_state.history = pd.DataFrame()\n",
        "    st.session_state.is_in_anomaly = False\n",
        "    st.session_state.last_alert = None\n",
        "    st.session_state.event_log = []\n",
        "\n",
        "# --- Auto-Refresh Component ---\n",
        "if st.session_state.running:\n",
        "    st_autorefresh(interval=2000, limit=None, key=\"auto_refresher\")\n",
        "\n",
        "# --- Sidebar ---\n",
        "st.sidebar.title(\"Configuration & Controls\")\n",
        "sku_list = [\"Chocolate Chip\", \"Oatmeal Raisin\"]\n",
        "selected_sku = st.sidebar.selectbox(\"Select Product (SKU)\", sku_list, index=sku_list.index(st.session_state.sku))\n",
        "if selected_sku != st.session_state.sku:\n",
        "    st.session_state.sku = selected_sku; st.session_state.running = False; st.session_state.step = 0; st.session_state.history = pd.DataFrame(); st.session_state.is_in_anomaly = False; st.session_state.last_alert = None; st.rerun()\n",
        "\n",
        "sku_data = load_sku_data(st.session_state.sku)\n",
        "st.sidebar.markdown(\"---\")\n",
        "button_label = \"‚ñ∂Ô∏è Start\"\n",
        "if st.session_state.running: button_label = \"Running...\"\n",
        "elif 0 < st.session_state.step < len(sku_data['live_demo_batch']): button_label = \"‚ñ∂Ô∏è Resume\"\n",
        "if st.sidebar.button(button_label, key=\"start_resume\", disabled=st.session_state.running): st.session_state.running = True; st.rerun()\n",
        "if st.sidebar.button(\"‚èπÔ∏è Stop\", key=\"stop\"): st.session_state.running = False; st.rerun()\n",
        "if st.sidebar.button(\"üîÅ Reset\", key=\"reset\"): st.session_state.running = False; st.session_state.step = 0; st.session_state.history = pd.DataFrame(); st.session_state.is_in_anomaly = False; st.session_state.last_alert = None; st.rerun()\n",
        "\n",
        "# --- Main App Logic ---\n",
        "if st.session_state.running and st.session_state.step < len(sku_data['live_demo_batch']):\n",
        "    st.session_state.step += 1\n",
        "\n",
        "current_step_index = st.session_state.step -1 if st.session_state.step > 0 else 0\n",
        "current_data = sku_data['live_demo_batch'].iloc[current_step_index:current_step_index+1]\n",
        "if st.session_state.step > 0 and st.session_state.history.empty:\n",
        "    st.session_state.history = sku_data['live_demo_batch'].iloc[0:st.session_state.step]\n",
        "elif st.session_state.running:\n",
        "    st.session_state.history = sku_data['live_demo_batch'].iloc[0:st.session_state.step]\n",
        "\n",
        "# --- Main App Display ---\n",
        "st.title(f\"üè≠ Predictive Maintenance System: {st.session_state.sku}\")\n",
        "main_tabs = st.tabs([\"üìä Live Dashboard\", \"üìÇ Event Log & Reporting\"])\n",
        "\n",
        "with main_tabs[0]:\n",
        "    status_placeholder = st.empty()\n",
        "    st.markdown(\"---\")\n",
        "    col1, col2 = st.columns([1, 2])\n",
        "\n",
        "    t2_score = (models['scaler'].transform(current_data[models['features']]) @ models['inv_cov_matrix'] @ models['scaler'].transform(current_data[models['features']]).T)[0][0]\n",
        "    health_score = calculate_health_score(t2_score, models['t2_threshold'])\n",
        "    if health_score < 70 and not st.session_state.is_in_anomaly:\n",
        "        st.session_state.is_in_anomaly = True; st.session_state.last_alert = get_alert_details(current_data); st.session_state.running = False\n",
        "    elif health_score >= 70: st.session_state.is_in_anomaly = False\n",
        "\n",
        "    with col1:\n",
        "        fig_gauge = go.Figure(go.Indicator(mode=\"gauge+number\", value=health_score, title={'text': \"Process Health Score\"}, gauge={'axis': {'range': [None, 100]}, 'bar': {'color': \"green\" if health_score > 80 else \"orange\" if health_score > 65 else \"red\"}}))\n",
        "        fig_gauge.update_layout(height=250, margin=dict(l=10, r=10, b=10, t=50, pad=4)); st.plotly_chart(fig_gauge, use_container_width=True)\n",
        "        predicted_moisture = models['xgb_model'].predict(current_data[models['features']])[0]\n",
        "        st.metric(\"üíß Predicted Moisture\", f\"{predicted_moisture:.2f} %\", f\"{predicted_moisture - 8.0:.2f} vs Target\")\n",
        "        st.metric(\"üìà Anomaly Score (T¬≤)\", f\"{t2_score:.2f}\", f\"Threshold: {models['t2_threshold']:.1f}\")\n",
        "\n",
        "    with col2:\n",
        "        history = st.session_state.history\n",
        "        if not history.empty:\n",
        "            history['Health Score'] = history.apply(lambda row: calculate_health_score((models['scaler'].transform(row[models['features']].values.reshape(1, -1)) @ models['inv_cov_matrix'] @ models['scaler'].transform(row[models['features']].values.reshape(1, -1)).T)[0][0], models['t2_threshold']), axis=1)\n",
        "            anomaly_points = history[history['Health Score'] < 70]\n",
        "            fig_chart = go.Figure(); fig_chart.add_trace(go.Scatter(x=history['timestamp'], y=history['Health Score'], mode='lines', name='Health Score', line=dict(color='royalblue')))\n",
        "            if not anomaly_points.empty: fig_chart.add_trace(go.Scatter(x=anomaly_points['timestamp'], y=anomaly_points['Health Score'], mode='markers', name='Anomaly', marker=dict(color='red', size=10, symbol='x')))\n",
        "            fig_chart.update_layout(title=\"Health Score Over Time\", yaxis_range=[0,105]); st.plotly_chart(fig_chart, use_container_width=True)\n",
        "\n",
        "    if st.session_state.last_alert:\n",
        "        with st.expander(\"üö® ALERT: Corrective Action Required\", expanded=True):\n",
        "             # --- THIS IS THE FIX ---\n",
        "             st.error(st.session_state.last_alert['impact']); st.warning(st.session_state.last_alert['cause'])\n",
        "             # --- END OF FIX ---\n",
        "             with st.form(\"playbook_form\"):\n",
        "                 responses = {step: st.checkbox(label=st.session_state.last_alert['playbook'][step]) for step in st.session_state.last_alert['playbook']}\n",
        "                 submitted = st.form_submit_button(\"Acknowledge & Log Event\")\n",
        "                 if submitted and all(responses.values()):\n",
        "                     st.session_state.event_log.append({\"Timestamp\": pd.Timestamp.now(), \"SKU\": st.session_state.sku, \"Alert Cause\": st.session_state.last_alert['cause'], \"Quality Impact\": st.session_state.last_alert['impact'], \"Actions Taken\": \", \".join([st.session_state.last_alert['playbook'][s] for s,r in responses.items() if r]), \"Acknowledged By\": \"Operator_01\"})\n",
        "                     st.success(\"Event logged successfully!\"); st.session_state.last_alert = None; time.sleep(1); st.rerun()\n",
        "                 elif submitted: st.error(\"Please complete all playbook steps before logging.\")\n",
        "\n",
        "    with st.expander(\"Deep Dive: Process Parameter Charts\", expanded=True):\n",
        "        if not st.session_state.history.empty:\n",
        "            param_tabs = st.tabs([\"Temperature\", \"Airflow\", \"Pressure\", \"Raw Material\"])\n",
        "            with param_tabs[0]:\n",
        "                fig = go.Figure(); fig.add_trace(go.Scatter(x=history['timestamp'], y=history['temp_actual'], name=\"Live\", line=dict(color='red'))); fig.add_trace(go.Scatter(x=history['timestamp'], y=history['temp_setpoint'], name=\"Setpoint\", line=dict(color='gray', dash='dash'))); fig.add_trace(go.Scatter(x=history['timestamp'], y=sku_data['golden_batch_df']['temp_actual'], name=\"Golden Batch\", line=dict(color='gold', dash='dot'))); st.plotly_chart(fig, use_container_width=True)\n",
        "            with param_tabs[1]:\n",
        "                fig = go.Figure(); fig.add_trace(go.Scatter(x=history['timestamp'], y=history['airflow_actual'], name=\"Live\", line=dict(color='deepskyblue'))); fig.add_trace(go.Scatter(x=history['timestamp'], y=history['airflow_setpoint'], name=\"Setpoint\", line=dict(color='gray', dash='dash'))); fig.add_trace(go.Scatter(x=history['timestamp'], y=sku_data['golden_batch_df']['airflow_actual'], name=\"Golden Batch\", line=dict(color='gold', dash='dot'))); st.plotly_chart(fig, use_container_width=True)\n",
        "            with param_tabs[2]:\n",
        "                fig = go.Figure(); fig.add_trace(go.Scatter(x=history['timestamp'], y=history['pressure_actual'], name=\"Live\", line=dict(color='blueviolet'))); fig.add_trace(go.Scatter(x=history['timestamp'], y=sku_data['golden_batch_df']['pressure_actual'], name=\"Golden Batch\", line=dict(color='gold', dash='dot'))); st.plotly_chart(fig, use_container_width=True)\n",
        "            with param_tabs[3]:\n",
        "                fig = go.Figure(); fig.add_trace(go.Scatter(x=history['timestamp'], y=history['raw_material_protein'], name=\"Live\", line=dict(color='brown'))); fig.add_trace(go.Scatter(x=history['timestamp'], y=sku_data['golden_batch_df']['raw_material_protein'], name=\"Golden Batch\", line=dict(color='gold', dash='dot'))); st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    if st.session_state.running: status_placeholder.header(f\"‚ñ∂Ô∏è Running... (Health: {health_score:.0f}%)\")\n",
        "    elif st.session_state.step >= len(sku_data['live_demo_batch']): status_placeholder.success(\"‚úÖ Simulation Finished.\")\n",
        "    elif st.session_state.is_in_anomaly: status_placeholder.error(\"üö® Simulation Paused on Anomaly. Review details and complete playbook.\")\n",
        "    elif st.session_state.step > 0: status_placeholder.info(\"‚ÑπÔ∏è Simulation is paused. Press 'Resume' to continue.\")\n",
        "    else: status_placeholder.info(\"‚ÑπÔ∏è Simulation is ready. Press 'Start' to begin.\")\n",
        "\n",
        "with main_tabs[1]:\n",
        "    st.header(\"üìÇ Historical Event Log\")\n",
        "    if not st.session_state.event_log: st.info(\"No events have been logged during this session.\")\n",
        "    else:\n",
        "        log_df = pd.DataFrame(st.session_state.event_log).sort_values(by=\"Timestamp\", ascending=False)\n",
        "        st.dataframe(log_df, use_container_width=True)\n",
        "        csv = log_df.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(label=\"üì• Download Log as CSV\", data=csv, file_name='anomaly_event_log.csv', mime='text/csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hctneAiBLi0n",
        "outputId": "5012d275-ae25-49a2-a882-e44e30d9c028"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if any\n",
        "ngrok.kill()\n",
        "\n",
        "# Set your ngrok authtoken\n",
        "NGROK_AUTH_TOKEN = \"31eXTJ1vwYC80Mb3tqVZ2y65uqT_7o1EHT2PgnjYURTVzj4iw\"  # Replace with your token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Run streamlit in background\n",
        "!nohup streamlit run app.py --server.port 8501 &\n",
        "\n",
        "# Open a tunnel to the streamlit port\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Click the URL to open the app: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc8ST35sNALG",
        "outputId": "9bfb1d2b-7f7b-4516-8ea9-37e1a9450e05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Click the URL to open the app: NgrokTunnel: \"https://f891ee8325b4.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "C2srUzcx_1LA"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}